[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "What’s New & Updated",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nJacobi method: From a naïve implementation to a modern Fortran multithreaded one\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\nJean-Christophe Loiseau\n\n\n\n\n\n\n\n\n\n\n\n\nIs Fortran better than Python for teaching the basics of numerical linear algebra?\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\nJean-Christophe Loiseau\n\n\n\n\n\n\n\n\n\n\n\n\nFirst steps with Elan, an educational programming language\n\n\n\nblog\n\n\n\n\n\n\n\n\n\nSep 9, 2025\n\n\nJean-Christophe Loiseau\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Total citations as of September 2025 from Google Scholar \n\n\nCitations\nh-index\ni-10 index\n\n\n\n\n2229\n20\n26\n\n\n\nList of publications in reversed chronological order (most recent first). For each, a link to the journal version is given along with the corresponding arXiv preprint (if not open access).\nNotes : * corresponding author; GS graduate student advisee; VS visiting scholar.\n\n\nR. A. S. FrantzGS*, C. Mimeau, M. SalihogluGS, J.-Ch. Loiseau, & J.-Ch. Robinet (2025). Bifurcation sequence in the wakes of a sphere and a cube. Journal of Fluid Mechanics, vol. 1018. [PDF] \n\n\n\n\n\n\nJ. L. Callaham*GS, J.-Ch. Loiseau, & S. L. Brunton (2023). Multiscale model reduction for incompressible flows. Journal of Fluid Mechanics, vol. 973. [PDF] \n\n\n\n\n\n\nR. A. S. FrantzGS*, J.-Ch. Loiseau, & J.-Ch. Robinet (2023). Krylov methods for large-scale dynamical systems: applications in fluid dynamics. Applied Mechanics Review, 75(3). [PDF] [arXiv] \n\n\n\n\n\n\nM. A. ElhawayGS, F. Romano, J.-Ch. Loiseau, & A. Dazin* (2023). Machine learning for optimal flow control in an axial compressor. European Physical Journal E., 46(28). [PDF] \n\n\n\n\n\n\nG. Nastro*, J.-Ch. Robinet, J.-Ch. Loiseau, P.-Y. Passaggia, & N. Mazellier (2023). Global stability, sensitivity and passive control of low-Reynolds number flows around NACA 4412 swept wings. Journal of Fluid Mechanics, vol. 957. [PDF] \n\n\n\n\n\n\nO. ChehabGS, A. Défossez, J.-Ch. Loiseau, A. Gramfort, & J.-R. King* (2022). Deep recurrent encoder: an end-to-end network to model magnetoencephalography at scale. Neurons, Behavior, Data Analysis and Theory. [PDF] \n\n\n\n\n\n\nJ. L. CallahamGS*, G. Rigas, J.-Ch. Loiseau, & S. L. Brunton (2022). An empirical mean-field model of symmetry-breaking in a turbulent wake. Science Advances, 8(19). [PDF] \n\n\n\n\n\n\nA. Sansica*, J.-Ch. Loiseau, M. Kanamori, A. Hashimoto, & J.-Ch. Robinet (2022). System identification of two-dimensional transonic buffet. AIAA Journal, 60(5). [PDF] \n\n\n\n\n\n\nJ. L. CallahamGS*, S. L. Brunton, & J.-Ch. Loiseau (2022). On the role of nonlinear correlations in reduced-order modelling. Journal of Fluid Mechanics, vol. 938. [PDF] \n\n\n\n\n\n\nA. A. KaptanogluGS*, B. M. de SilvaGS, U. Fasel, K. KahemanGS, A. J. Goldschmidt, J. CallahamGS, C. B. Delahunt, Z. G. Nicolaou, K. ChampionGS, J.-Ch. Loiseau, J. N. Kutz, & S. L. Brunton (2022). PySINDy: a comprehensive Python package for robust sparse system identification. Journal of Open Source Software, 7(69). [PDF] \n\n\n\n\n\n\nJ. L. CallahamGS*, J.-Ch. Loiseau, G. Rigas, & S. L. Brunton (2021). Nonlinear stochastic modelling with Langevin regression. Proceedings of the Royal Society A, 477:20210092. [PDF] \n\n\n\n\n\n\nM. A. BucciGS*, S. Cherubini, J.-Ch. Loiseau, & J.-Ch. Robinet (2021). Influence of freestream turbulence on the flow over a wall roughness. Physical Review Fluids, vol. 6. [PDF] \n\n\n\n\n\n\nJ.-Ch. Loiseau* (2020). Data-driven modeling of the chaotic thermal convection in an annular thermosyphon. Theoretical and Computational Fluid Dynamics, 34(4). [PDF] \n\n\n\n\n\n\nB. M. de SilvaGS*, K. ChampionGS, M. Quade, J.-Ch. Loiseau, J. N. Kutz, & S. L. Brunton (2020). PySINDy: a Python package for the sparse identification of nonlinear dynamical systems from data. Journal of Open Source Software, 5(49). [PDF] \n\n\n\n\n\n\nY. BenganaGS, J.-Ch. Loiseau, J.-Ch. Robinet, & L. S. Tuckerman* (2019). Bifurcation analysis and frequency prediction in shear-driven cavity flow. Journal of Fluid Mechanics, vol. 875. [PDF] \n\n\n\n\n\n\nS. GhoshGS*, J.-Ch. Loiseau, W.-P. Breugem, & L. Brandt (2019). Modal and non-modal linear stability of Poiseuille flow through a channel with a porous substrate. European Journal of Mechanics - B/Fluids, vol. 75. [PDF] \n\n\n\n\n\n\nF. PicellaGS*, J.-Ch. Loiseau, F. Lusseyran, J.-Ch. Robinet, S. Cherubini, & L. Pastur (2018). Successive bifurcations in a fully three-dimensional open cavity flow. Journal of Fluid Mechanics, vol. 844. [PDF] \n\n\n\n\n\n\nJ.-Ch. Loiseau*, B. R. Noack, & S. L. Brunton (2018). Sparse reduced-order modelling: sensor-based dynamics to full-state estimation. Journal of Fluid Mechanics, vol. 844 [PDF] \n\n\n\n\n\n\nJ.-Ch. Loiseau* and S. L. Brunton (2018). Constrained sparse Galerkin regression. Journal of Fluid Mechanics, vol. 838. [PDF] \n\n\n\n\n\n\nM. A. BucciGS*, D. K. PuckertGS, C. A. AndrianoGS, J.-Ch. Loiseau, S. Cherubini, J.-Ch. Robinet, & U. Rist (2017). Roughness-induced transition by quasi-resonance of a varicose global mode. Journal of Fluid Mechanics, vol. 836. [PDF] \n\n\n\n\n\n\nZ. GeGS*, J.-Ch. Loiseau, O. Tammisola, & L. Brandt (2018). An efficient mass-preserving interface-correction level set/ghost fluid method for droplet suspensions under depletion forces. Journal of Computational Physics, vol. 353. [PDF] \n\n\n\n\n\n\nO. Tammisola, J.-Ch. Loiseau, & L. Brandt (2017). Effect of viscosity ratio on the self-sustained instabilities in planar immiscible jets. Physical Review Fluids, vol. 2. [PDF] \n\n\n\n\n\n\nA. A. BanaeiGS, J.-Ch. Loiseau, I. LashgariGS, & L. Brandt (2017). Numerical simulations of elastic capsules with nucleus in shear flow. European Journal of Computational Mechanics, vol. 26. [PDF] \n\n\n\n\n\n\nJ.-Ch. Loiseau*, J.-Ch. Robinet, & E. Leriche (2016). Intermittency and transition to chaos in the cubical lid-driven cavity flow. Fluid Dynamics Research, vol. 48. [PDF] \n\n\n\n\n\n\nA. Ducoin*, J.-Ch. Loiseau, & J.-Ch. Robinet (2016). Numerical investigation of the interaction between laminar to turbulent transition and the wake of an airfoil. European Journal of Mechanics - B/Fluids, vol. 57. [PDF] \n\n\n\n\n\n\nJ. FanonGS, J.-Ch. Loiseau, P. Valluri, I. Bethune, & L. O Naraigh (2016). High-performance computational fluid dynamics: a custom-code approach. European Journal of Physics, vol. 37. [PDF] \n\n\n\n\n\n\nJ.-Ch. Loiseau*, J.-Ch. Robinet, S. Cherubini, & E. Leriche (2014). Investigation of the roughness-induced transition: global stability analyses and direct numerical simulations. Journal of Fluid Mechanics, vol. 760. [PDF]"
  },
  {
    "objectID": "publications.html#selected-work",
    "href": "publications.html#selected-work",
    "title": "Publications",
    "section": "Selected Work",
    "text": "Selected Work\nEinstein, A., Podolsky, B., & Rosen, N. (1935). Can quantum-mechanical description of physical reality be considered complete?. Physical review, 47(10), 777. [pdf] [code and data]   \n\n\n\n\n\n\nEinstein, A. (1965). Concerning an heuristic point of view toward the emission and transformation of light. American Journal of Physics, 33(5), 367. [pdf]"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n      \n      \n        Title\n      \n      \n        Team\n      \n      \n        Funder\n      \n      \n        Description\n      \n      \n        Start\n      \n      \n        End\n      \n      \n        Funding\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nTitle\n\n\n\nTeam\n\n\n\nFunder\n\n\n\nDescription\n\n\n\nFunding\n\n\n\nStart\n\n\n\nEnd\n\n\n\n\n\n\n\n\n\n\n\nExploring \n\n\nAcademic Website\n\n\nFunder\n\n\nThe goal of this project is to investigate .\n\n\n$0\n\n\n2024\n\n\n2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/news-title/index.html",
    "href": "posts/news-title/index.html",
    "title": "Migrating from Jekyll & AcademicPages to Quarto",
    "section": "",
    "text": "About the news"
  },
  {
    "objectID": "posts/news-title/index.html#summary",
    "href": "posts/news-title/index.html#summary",
    "title": "Migrating from Jekyll & AcademicPages to Quarto",
    "section": "",
    "text": "About the news"
  },
  {
    "objectID": "posts/paper-title/index.html#abstract",
    "href": "posts/paper-title/index.html#abstract",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Abstract",
    "text": "Abstract\nIn a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete."
  },
  {
    "objectID": "posts/paper-title/index.html#links",
    "href": "posts/paper-title/index.html#links",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Links",
    "text": "Links\nPublished paper"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Program\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nMaster 2 MFFA\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nProgramme Ingénieur de Spécialité\nSpring\nFrench\n\n\n\nDescription –\nPre-requisites –"
  },
  {
    "objectID": "teaching.html#class-1",
    "href": "teaching.html#class-1",
    "title": "Teaching",
    "section": "",
    "text": "Fall 2024,   Fall 2023\nClass 1 introduction."
  },
  {
    "objectID": "teaching.html#class-2",
    "href": "teaching.html#class-2",
    "title": "Teaching",
    "section": "Class 2",
    "text": "Class 2\n\n\n\n\n\nSpring 2025,   Fall 2023\nClass 2 introduction."
  },
  {
    "objectID": "teaching.html#class-3",
    "href": "teaching.html#class-3",
    "title": "Teaching",
    "section": "Class 3",
    "text": "Class 3\n\n\n\n\n\nSpring 2025,   Spring 2024\nClass 3 introduction."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Dr. Jean-Christophe Loiseau\nLaboratoire DynFluid\nEcole Nationale Supérieure d’Arts et Métiers\nAddress: 151 boulevard de l’hôpital, 75013 Paris, France\nE-mail: jean-christophe.loiseau at ensam dot eu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "",
    "text": "E-mail\n  \n  \n    \n     GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n      ORCID\n  \n  \n      Google Scholar\n  \n  \n      Research Gate\n  \n\n  \n  \n\n\n\n\n\n\nWarning\n\n\n\nSite still under construction. Bare with me.\n\n\n\n\nI’m an Assitant Professor of applied mathematics and fluid dynamics at Arts et Métiers Institute of Technology (Paris, France). I split my time between teaching to (mechanical) engineering students and doing research on topics such as transition to turbulence, flow control and data-driven modelling.\nOn this site I keep a list of my publications, the courses I teach, the open-source softwares I develop or contribute to, as well as a technical blog.\n\n\n\n PySINDy – Sparse identification of nonlinear dynamical systems from data.\n LightKrylov – Implementation of Krylov methods in modern Fortran.\n LightConvex – Convex programming in modern Fortran.\n stdlib – Community-driven development of a standard library for Fortran."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "",
    "text": "Researcher, Univerisity"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "",
    "text": "PhD, Univerisity"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "Recent Posts",
    "text": "Recent Posts\nCheck out the latest  Papers ,  News ,  Events , and  More »\n\n\n\n\n\n\n\n\n\n\nJacobi method: From a naïve implementation to a modern Fortran multithreaded one\n\n\n\nSep 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs Fortran better than Python for teaching the basics of numerical linear algebra?\n\n\n\nSep 9, 2025\n\n\n\n\n\n\nNo matching items\n\n\nAll Posts »"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Postdoc\n\n\n\n\n\n\n\n\n\n\n\n\n\nP.I.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#team",
    "href": "people.html#team",
    "title": "People",
    "section": "",
    "text": "Postdoc\n\n\n\n\n\n\n\n\n\n\n\n\n\nP.I.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "people.html#alumni",
    "href": "people.html#alumni",
    "title": "People",
    "section": "Alumni",
    "text": "Alumni\n\n\n   \n    \n    \n      Order By\n      Default\n      \n      \n      \n        Name\n      \n      \n        Role\n      \n      \n        Started\n      \n      \n        Ended\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n \n\n\n\nName\n\n\n\nRole\n\n\n\nStarted\n\n\n\nEnded\n\n\n\n\n\n\n\n\n\n\n\nMohamed Elhawary\n\n\nPhD Alumni\n\n\n2024\n\n\n2021\n\n\n\n\n\n\n\n\n\nRicardo Schuh Frantz\n\n\nPhD Alumni\n\n\n2019\n\n\n2022\n\n\n\n\n\n\n\n\n\nCosimo Tarcia Morisco\n\n\nPhD Alumni\n\n\n2020\n\n\n2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "",
    "text": "I’m an Assitant Professor of applied mathematics and fluid dynamics at Arts et Métiers Institute of Technology (Paris, France). I split my time between teaching to (mechanical) engineering students and doing research on topics such as transition to turbulence, flow control and data-driven modelling.\nOn this site I keep a list of my publications, the courses I teach, the open-source softwares I develop or contribute to, as well as a technical blog."
  },
  {
    "objectID": "posts/blog-title/index.html",
    "href": "posts/blog-title/index.html",
    "title": "Blog posts",
    "section": "",
    "text": "About the blog"
  },
  {
    "objectID": "posts/blog-title/index.html#summary",
    "href": "posts/blog-title/index.html#summary",
    "title": "Blog posts",
    "section": "",
    "text": "About the blog"
  },
  {
    "objectID": "posts/paper-title/einstein.html#abstract",
    "href": "posts/paper-title/einstein.html#abstract",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Abstract",
    "text": "Abstract\nIn a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete."
  },
  {
    "objectID": "posts/paper-title/einstein.html#links",
    "href": "posts/paper-title/einstein.html#links",
    "title": "Can quantum-mechanical description of physical reality be considered complete?",
    "section": "Links",
    "text": "Links\nPublished paper"
  },
  {
    "objectID": "posts/paper-title/test.html#abstract",
    "href": "posts/paper-title/test.html#abstract",
    "title": "This is a test?",
    "section": "Abstract",
    "text": "Abstract\nIn a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete."
  },
  {
    "objectID": "posts/paper-title/test.html#links",
    "href": "posts/paper-title/test.html#links",
    "title": "This is a test?",
    "section": "Links",
    "text": "Links\nPublished paper"
  },
  {
    "objectID": "teaching.html#mathematics-for-engineers",
    "href": "teaching.html#mathematics-for-engineers",
    "title": "Teaching",
    "section": "",
    "text": "Fall 2024,   Fall 2023\nClass 1 introduction."
  },
  {
    "objectID": "teaching.html#introduction-to-control-theory",
    "href": "teaching.html#introduction-to-control-theory",
    "title": "Teaching",
    "section": "Introduction to control theory",
    "text": "Introduction to control theory\n\n\n\n\n\nSpring 2025,   Fall 2023\nClass 2 introduction."
  },
  {
    "objectID": "teaching.html#arts-et-métiers",
    "href": "teaching.html#arts-et-métiers",
    "title": "Teaching",
    "section": "",
    "text": "Program\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nInternational Master Program Factory of the Future\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nMaster 2 MFFA\nFall\nEnglish\n\n\n\nDescription –\nPre-requisites –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nProgramme Ingénieur de Spécialité\nSpring\nFrench\n\n\n\nDescription –\nPre-requisites –"
  },
  {
    "objectID": "teaching.html#université-de-la-rochelle",
    "href": "teaching.html#université-de-la-rochelle",
    "title": "Teaching",
    "section": "Université de La Rochelle",
    "text": "Université de La Rochelle\n\nPhysics Informed Neural Networks and System Identification\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nMaster 2 MIX\nFall\nFrench\n\n\n\nDescription –\nPre-requisites –"
  },
  {
    "objectID": "teaching.html#sorbonne-université",
    "href": "teaching.html#sorbonne-université",
    "title": "Teaching",
    "section": "Sorbonne Université",
    "text": "Sorbonne Université\n\nIntroduction to Machine Learning\n\n\n\n\n\n\n\n\n\nProgram\nPeriod\nLanguage\n\n\n\n\nMaster 2 MFFA\nFall\nFrench\n\n\n\nDescription –\nPre-requisites –"
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Jean-Christophe Loiseau, PhD",
    "section": "",
    "text": "PySINDy – Sparse identification of nonlinear dynamical systems from data.\n LightKrylov – Implementation of Krylov methods in modern Fortran.\n LightConvex – Convex programming in modern Fortran.\n stdlib – Community-driven development of a standard library for Fortran."
  },
  {
    "objectID": "posts/blog-title/fortran_vs_python.html",
    "href": "posts/blog-title/fortran_vs_python.html",
    "title": "Is Fortran better than Python for teaching the basics of numerical linear algebra?",
    "section": "",
    "text": "Disclaimer – This is not a post about which language is the most elegant or which implementation is the fastest (we all know it’s Fortran). It’s about teaching the basics of scientific computing to engineering students with a limited programming experience. Yes, the Numpy/Scipy/matplotlib stack is awesome. Yes, you can use numba or jax to speed up your code, or Cython, or even Mojo the latest kid in the block. Or you know what? Use Julia or Rust instead. But that’s not the basics and it’s beyond the point.\nI’ve been teaching an Intro to Scientific Computing class for nearly 10+ years. This class is intended for second year engineering students and, as such, places a large emphasis on numerical linear algebra. Like the rest of Academia, I’m using a combination of Python and numpy arrays for this. Yet, after all these years, I start to believe it ain’t necessarily the right choice for a first encounter with numerical linear algebra. Obvisouly everything is not black and white and I’ll try to be nuanced. But, in my opinion, a strongly typed language such as Fortran might lead to an overall better learning experience. And that’s what it’s all about when you start Uni: learning the principles of scientific programming, not the quirks of a particular language (unless you’re a CS student, which is a different crowd).\nDon’t get me wrong though. Being proficient with numpy, scipy and matplotlib is an absolute necessity for STEM students today, and that’s a good thing. Even from an educational perspective, the scientific Python ecosystem enables students to do really cool projects, putting the fun back in learning. It would be completely non-sensical to deny this. But using x = np.linalg.solve(A, b) ain’t the same thing as having a basic understanding of how these algorithms work. And to be clear: the goal of these classes is not to transform a student into a numerical linear algebra expert who could write the next generation LAPACK. It is to teach them just enough of numerical computing so that, when they’ll transition to an engineering position, they’ll be able to make an informed decision regarding which solver or algorithm to use when writing a simulation or data analysis tool to tackle whatever business problem they’re working on.\nIf you liked and aced your numerical methods class, then what I’ll discuss might not necessary be relatable. You’re one of a kind. More often than not, students struggle with such courses. This could be due to genuine comprehension difficulties, or lazyness and lack of motivation simply because they don’t see the point. While both issues are equally important to address, I’ll focus on the first one: students who are willing to put the effort into learning the subject but have difficulties transforming the mathematical algorithm into an actionnable piece of code. Note however that initially motivated but struggling students might easily drift to the second type, hence my focus there first.\nIn the rest of this post, I’ll go through two examples. For each, I’ll show a typical Python code such a student might write and discuss all of the classical problems they’ve encountered to get there. A large part of these are syntax issues or result from the permissiveness of an interpreted language like Python which is a double edged sword. Then I’ll show an equivalent Fortran implementation and explain why I believe it can solve part of these problems. But first, I need to address the two elephants in the room:\nWith that being said, let’s get started with a concrete, yet classical, example to illustrate my point."
  },
  {
    "objectID": "posts/blog-title/fortran_vs_python.html#the-hello-world-of-iterative-solvers",
    "href": "posts/blog-title/fortran_vs_python.html#the-hello-world-of-iterative-solvers",
    "title": "Is Fortran better than Python for teaching the basics of numerical linear algebra?",
    "section": "The Hello World of iterative solvers",
    "text": "The Hello World of iterative solvers\nYou’ve started University a year ago and are taking your first class on scientific computing. Maybe you already went through the hassle of Gaussian elimination and the LU factorization. During the last class, Professor X discussed about iterative solvers for linear systems. It is now the hands-on session and today’s goal is to implement the Jacobi method. Why Jacobi? Because it is simple enough to implement in an hour or so.\nThe exact problem you’re given is the following:\n\nConsider the Poisson equation with homogeneous Dirichlet boundary conditions on the unit-square. Assume the Laplace operator has been discretized using a second-order accurate central finite-difference scheme. The discretized equation reads \\[\\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\\Delta x^2} + \\dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\\Delta y^2} = b_{i, j}.\\] For the sake of simplicity, take \\(\\Delta x = \\Delta y\\). Write a function implementing the Jacobi method to solve the resulting linear system to a user-prescribed tolerance.\n\nWe can all agree this is a simple enough yet somewhat realistic example. More importantly, it is sufficient to illustrate my point. Here is what the average student might write in Python.\nimport numpy as np\n\ndef jacobi(b , dx, tol, maxiter):\n    # Initialize variables.\n    nx, ny = b.shape\n    residual = 1.0\n    u = np.zeros((nx, ny))\n    tmp = np.zeros((nx, ny))\n\n    # Jacobi solver.\n    for iteration in range(maxiter):\n        # Jacobi iteration.\n        for i in range(1, nx-1):\n            for j in range(1, ny-1):\n                tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] \n                                                - u[i, j+1] - u[i, j-1])\n\n        # Compute residual\n        residual = np.linalg.norm(u-tmp)\n        # Update solution.\n        u = tmp\n        # If converged, exit the loop.\n        if residual &lt;= tol:\n            break\n\n    return u\nYes, you shouldn’t do for loops in Python. But remember, you are not a seasoned programmer. You’re taking your first class on scientific computing and that’s how the Jacobi method is typically presented. Be forgiving.\n\nWhere do students struggle?\nAdmittidely, the code is quite readable and look very similar to the pseudocode you’d use to describe the Jacobi method. But if you’re reading this blog post, there probably are a handful of things you’ve internalized and don’t even think about anymore (true for both Python and Fortran). And that’s precisely what the students (at least mine) struggle with, starting with the very first line.\nWhat the hell is numpy and why do I need it? Also, why import it as np? – These questions come back every year. Yet, I don’t have satisfying answers. I always hesitate between\n\nTrust me kid, you don’t want to use nested lists in Python to do any serious numerical computing.\n\nwhich naturally begs the question of why, or\n\nWhen I said we’ll use Python for this scientific computing class, what I really meant is we’ll use numpy which is a package written for numerical computing because Python doesn’t naturally have good capabilities for number crunching. As for the import as np, that’s just a convention.\n\nAnd this naturally leads to the question of “why Python in the first place then?” for which the only valid answer I have is\n\nWell, because Python is supposed to be easy to learn and everybody uses it.\n\nClearly, import numpy as np is an innocent-looking line of code. It has nothing to do with the subject being taught though, and everything with the choice of the language, only diverting the students from the learning process.\nI coded everything correctly, 100% sure, but I get this weird error message about indentation – Oh boy! What a classic! The error message varies between\nIndentationError: expected an indented block\nand\nTabError: inconsistent use of tabs and spaces in indentation\n&lt;TAB&gt; versus SPACE is a surprisingly hot topic in programming which I don’t want to engage in. A seasoned programmer might say “simply configure your IDE properly” which is fair. But we’re talking about your average student (who’s not a CS one remember) and they might use IDLE or even just notepad. As for the IndentationError, it is a relatively easy error to catch. Yet, the fact that for, if or while constructs are not clearly delineated in Python other than visually is surprisingly hard for students. I find that it puts an additional cognitive burden on top of a subject which is already demanding enough.\nIt could also be more subtle. The code might run but the results are garbage because the student wrote something like\n    for iteration in range(maxiter):\n    # Jacobi iteration.\n    for i in range(1, nx-1):\n    for j in range(1, ny-1):\n    tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] \n                                                - u[i, j+1] - u[i, j-1])\nYou might argue that this perfectly understandable, though if you want to be picky, there is no dealineation of where the different loops end. Which the whole point of indentation in Python. But students do not necessarily get that.\nWhy range(1, nx-1) and not range(2, nx-1)? The first column/row is my boundary. – Another classic related to 0-based vs 1-based indexing. And another very hot debate I don’t want to engage in. The fact however is that linear algebra (and a lot of scientific computing for that matter) use 1-based indexing. Think about vectors or matrices. Almost every single maths books write them as\n\\[\n\\begin{bmatrix}\n    a_{11} & a_{12} & a_{13} \\\\\n    a_{21} & a_{22} & a_{23} \\\\\n    a_{31} & a_{32} & a_{33}\n\\end{bmatrix}.\n\\]\nThe upper left element has the (1, 1) index, not (0, 0). Why use a language with 0-based indexing for linear algebra other than putting an additional cognitive burden on the students learning the subject? This is a recipe for the nefarious off-by-one error. And these errors are sneaky. The code might run but produce incorrect results and it’s a nightmare for the students (or the poor TA helping them) to figure out why.\nWhy np.linalg.norm and not just norm or np.norm? – This is one is related to my first point. When you’re used to it, you no longer question it. But you don’t know students then and, once more, I don’t have a really clear answer other than\n\nWell, linalg stand for linear algebra, and np.linalg is a collection of linear algebra related function. It is a submodule of numpy, the package I told you about before.\n\nGrouping like-minded functionalities into a dedicated submodule is definitely good practice, no question there. Discussing the architecture of numpy makes a lot of sense when students have to do a big project involving numerical computing but not strictly speaking about numerical computing. On the other hand, when it is their first numerical computing class (and possibly first with Python) I find it distracting. Again, it’s not a big thing really but still. And then you have to explain why np.det and np.trace are not part of np.linalg…\nOther common problems – There are other very common problems like using the wrong function or inconsistent use of lower- or upper-case for variables. Once you know Python is case-sensitive, this is mainly a concentration problem. No big deal there. But there is one last thing that tends to cause problems to distracted students and that has to do with the dynamic nature of Python. Nowhere in the code snippet is it clearly specified that b needs to be a two-dimensional np.array of real numbers nor that it shouldn’t be modified by the function. It is only implicit. And that can be a big problem for students when working with marginally more complicated algorithms. Sure enough, type annotation is a thing now in Python, but it still is pretty new and comparatively few people actually use them.\n\n\nWhat about Fortran?\nAlright, I’ve spent the last five minutes talking shit about Python but how does Fortran compare with it? Here is a typical implementation of the same function. I’ve actually digged it from my own set of archived homeworks I did 15+ years ago and hardly modified it.\nfunction jacobi(b, dx, tol, maxiter) result(u)\n    implicit none\n    real, dimension(:, :), intent(in) :: b\n    real, intent(in) :: dx, tol\n    integer, intent(in) :: maxiter\n    real, dimension(:, :), allocatable :: u\n    ! Internal variables.\n    real, dimension(:, :), allocatable :: tmp\n    integer :: nx, ny, i, j, iteration\n\n    ! Initialize variables.\n    nx = size(b, 1) ; ny = size(b, 2)\n    allocate(u(nx, ny), source = 0.0)\n    residual = 1.0\n\n    ! Jacobi solver.\n    do iteration = 1, maxiter\n        ! Jacobi iteration.\n        do j = 2, ny-1\n            do i = 2, nx-1\n                tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &\n                                                - u(i, j+1) - u(i, j-1))\n            enddo\n        enddo\n\n        ! Compute residual.\n        residual = norm2(u - tmp)\n        ! Update solution.\n        u = tmp\n        ! If convered, exit the loop.\n        if (residual &lt;= tol) exit\n    enddo\n\nend function\nNo surprise there. The task is sufficiently simple that both implementations are equally readable. If anything, the Fortran one is a bit more verbose. But in view of what I’ve just said about the Python code, I think it actually a good thing. Let me explain.\nDefinition of the variables – Fortran is a strongly typed language. Lines 2 to 8 are nothing but the definitions of the different variables used in the routine. While you might argue it’s a pain in the a** to write these, I think it can actually be very beneficial for students. Before even implementing the method, they have to clearly think about which variables are input, which are ouput, what are their types and dimensions. And to do so, they have to have at least a minimal understanding of the algorithm itself. Once it’s done, there are no more surprises (hopefully), and the contract between the code and the user is crystal clear. And more importantly, the effort put in clearly identifying the input and output of numerical algorithm usually pays off and leads to less error-prone process.\nBegining and end of the constructs – Fortran uses the do/end do (or enddo) construct, clearly specifying where the loop starts where it ends. The indentation used in the code snippet really is just a matter of style. In constrast to Python, writing\n    do j = 2, ny-1\n    do i = 2, nx-1\n    tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &\n                                    - u(i, j+1) - u(i, j-1))\n    enddo\n    enddo\ndoes not make the code any less readable and wouldn’t change a dime in terms of computations. It’s a minor thing, fair enough. But it instantly get rid of the IndentationError or TabError which are puzzling students. I may be wrong, but I believe it actually reduces the cognitive load associated with the programming language and let the students focus on the actual numerical linear algebra task.\nNo off-by-one error – By default, Fortran uses a 1-based indexing. No off-by-one errors, period.\nIntrinsic functions for basic scientific computations – While you have to use np.linalg.norm in Python to compute the norm of a vector, Fortran natively has the norm2 function for that. No external library required. If you want to be picky, you may say that norm2 is a weird name and that norm might be just fine.\nSome quirks of Fortran – All is not perfect though, starting with Line 2 and the implicit none statement. This is a historical remnant which is considered good practice by modern Fortran standards but not actually needed. Students being students, they will more likely than not ask questions about it although it has nothing to do with the subject of the class itself. Admittidely, it can be a bit cumbersome to explicitely define all the integers you use even if it’s just for a one-time loop. Likewise, there is the question of real vs double precision vs real(wp) (where wp is yet another variable you’ve defined somewhere). I don’t think it matters too much though when learning the basics of numerical linear algebra algorithms, although it certainly does when you start discussing about precision and performances."
  },
  {
    "objectID": "posts/blog-title/fortran_vs_python.html#linear-least-squares-your-first-step-into-machine-learning",
    "href": "posts/blog-title/fortran_vs_python.html#linear-least-squares-your-first-step-into-machine-learning",
    "title": "Is Fortran better than Python for teaching the basics of numerical linear algebra?",
    "section": "Linear least-squares, your first step into Machine Learning",
    "text": "Linear least-squares, your first step into Machine Learning\nAlright, let’s look at another example. Same class, later in the semester. Professor X now discusses over-determined linear systems and how it relates to least-squares, regression and basic machine learning applications. During the hands-on session, you’re given the following problem\n\nConsider the following unconstrained quadratic program \\[\\mathrm{minimize} \\quad \\| Ax - b \\|_2^2.\\] Write a least-squares solver based on the QR factorization of the matrix \\(A\\). You can safely assume that \\(A\\) is a tall matrix (i.e. \\(m &gt; n\\)).\n\nHere is what the typical Python code written by the students might look like.\nimport numpy as np\n\ndef qr(A):\n    # Initialize variables.\n    m, n = A.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    # QR factorization based on the Gram-Schmidt orthogonalization process.\n    for i in range(n):\n        q = A[:, i]\n        # Orthogonalization w.r.t. to the previous basis vectors.\n        for j in range(i):\n            R[j, i] = np.vdot(q, Q[:, j])\n            q = q - R[j, i]*Q[:, j]\n\n        # Normalize and store the new vector.\n        R[i, i] = np.linalg.norm(q)\n        Q[:, i] = q / R[i, i]\n\n    return Q, R\n\ndef upper_triangular_solve(R, b):\n    # Initialize variables.\n    n = R.shape[0]\n    x = np.zeros((n))\n\n    # Backsubstitution.\n    for i in range(n-1, -1, -1):\n        x[i] = b[i]\n        for j in range(n-1, i, -1):\n            x[i] = x[i] - R[i, j]*x[j]\n        x[i] = x[i] / R[i, i]\n\n    return x\n\ndef lstsq(A, b):\n    # QR factorization.\n    Q, R = qr(A)\n    # Solve R @ x = Q.T @ b.\n    x = upper_triangular_solve(R, Q.T @ b)\n    return x\nThis one was adapted from an exercise I gave last year. In reality, students lumped everything into one big function unless told otherwise, but nevermind. For comparison, here is the equivalent Fortran code.\nsubroutine qr(A, Q, R)\n    implicit none\n    real, dimension(:, :), intent(in) :: A\n    real, dimension(:, :), allocatable, intent(out) :: Q, R\n    ! Internal variables.\n    integer :: i, j, m, n\n    real, dimension(:), allocatable :: q_hat\n\n    ! Initialize variables.\n    m = size(A, 1); n = size(A, 2)\n    allocate(Q(m, n), source=0.0)\n    allocate(R(n, n), source=0.0)\n    \n    ! QR factorization based on the Gram-Schmidt orthogonalization process.\n    do i = 1, n\n        q_hat = A(:, i)\n        ! Orthogonalize w.r.t. the previous basis vectors.\n        do j = 1, i-1\n            R(j, i) = dot_product(q_hat, Q(:, j))\n            q_hat = q_hat - R(j, i)*Q(:, j)\n        end do\n\n        ! Normalize and store the new vector.\n        R(i, i) = norm2(q_hat)\n        Q(:, i) = q_hat / R(i, i)\n    end do\nend subroutine\n\nfunction upper_triangular_solve(R, b) result(x)\n    implicit none\n    real, dimension(:, :), intent(in) :: R\n    real, dimension(:), intent(in) :: b\n    real, dimension(:), allocatable :: x\n    ! Internal variables.\n    integer :: n, i, j\n\n    ! Initialize variables.\n    n = size(R, 1)\n    allocate(x(n), source=0.0)\n\n    ! Backsubstitution.\n    do i = n, 1, -1\n        x(i) = b(i)\n        do j = n-1, i, -1\n            x(i) = x(i) - R(i, j)*x(j)\n        enddo\n        x(i) = x(i) / R(i, i)\n    end do\nend function\n\nfunction lstsq(A, b) result(x)\n    implicit none\n    real, dimension(:, :), intent(in) :: A\n    real, dimension(:), intent(in) :: b\n    real, dimension(:), allocatable :: x\n    ! Internal variables.\n    real, dimension(:, :), allocatable :: Q, R\n\n    ! QR factorization.\n    call qr(A, Q, R)\n    ! Solve R @ x = Q.T @ b.\n    x = upper_triangular_solve(R, matmul(transpose(Q), b))\nend function\nJust like the Jacobi example, both implementations are equally readable. At this point in the semester, the students got somewhat more comfortable with Python. The classical indentation problems were not so much of a problem anymore. The off-by-one errors due to 0-based indexing for the Gram-Schmidt orthogonalization in qr or in the backsubstitution algorithm on the other hand… That was painful. In a 90-minutes class, it took almost a whole hour simply for them to debug these errors.\nBut there was another thing that confused students. A lot. And that has to do with computing dot products in numpy. There’s so many different ways: np.vdot(x, y), np.dot(x.T, y), np.dot(np.transpose(x), y), or x.transpose().dot(y) to list just the ones I have seen in their codes. Again, this has nothing to do with linear algebra, but everything with the language. Not only do they need to learn the math, but they simultaneously need to learn the not-quite-necessarily-math-standard syntax used in the language (yes, I’m looking at you @). It’s just a question of habits, sure enough, but again it can be impeding the learning process.\nOn the other hand, the Fortran implementation is even closer to the standard mathematical description of the algorithm: 1-based indexing, intrinsic dot_product function, etc. But beside the implicit none, there is the need to use a subroutine rather than a function construct for the QR decomposition because it has two output variables. Not a big deal again, but to be fair, it does add another minor layer of abstraction due to the language semantics rather than that of the subject being studied."
  },
  {
    "objectID": "posts/blog-title/fortran_vs_python.html#fortran-may-have-a-slight-edge-but-i-swept-some-things-under-the-rug",
    "href": "posts/blog-title/fortran_vs_python.html#fortran-may-have-a-slight-edge-but-i-swept-some-things-under-the-rug",
    "title": "Is Fortran better than Python for teaching the basics of numerical linear algebra?",
    "section": "Fortran may have a slight edge, but I swept some things under the rug…",
    "text": "Fortran may have a slight edge, but I swept some things under the rug…\nIn the end, when it comes to teaching the basics of numerical linear algebra, Python and Fortran are not that different. And in that regard, neither is Julia which I really like as well. The main advantages I see of using Fortran for this task however are:\n\n1-based indexing : in my experience, the 0-based indexing in Python leads to so many off-by-one erros driving the students crazy. Because linear algebra textbooks naturally use 1-based indexing, having to translate everything in your head to 0-based indices is a huge cognitive burden on top of a subject already demanding enough. You might get used to it eventually, but it’s a painful process impeding the learning outcomes.\nStrong typing : combined with implicit none, having to declare the type, dimension and input or output nature of every variable you use might seem cumbersome at first. But it forces students to pause and ponder to identify which is which. Sure this is an effort, but it is worth it. Learning is not effortless and this effort forces you to have a somewhat better understanding of a numerical algorithm before even starting to implement it. Which I think is a good thing.\nClear delineation of the constructs : at least during the first few weeks, having to rely only on visual clues to identify where does a loop ends in Python seems to be quite complicated for a non-negligible fraction of the students I have. In that respect, the do/enddo construct in Fortran is much more explicit and probably easier to grasp.\n\nObvisouly, I’m not expecting educators worldwide to switch back to Fortran overnight, nor is it necessarily desirable. The advantages I see are non-negligible from my perspective but certainly not enough by themselves. There are many other things that need to be taken into account. Python is a very generalist language. You can do so much more than just numerical computing so it makes complete sense to have it in the classroom. The ecosystem is incredibly vast and the interactive nature definitely has its pros. Notebooks such as Jupyter can be incredible teaching tools (although they come with their own problems in term good coding practices). So are the Pluto notebooks in Julia.\nFortran is good at one thing: enabling computational scientists and engineers to write high-performing mathematical models without all the intricacies of equally peformant but more CS-oriented languages such as C or C++. Sure enough, the modern Fortran ecosystem is orders of magnitude smaller than Python, and targetted toward numerical computing almost exclusively. And the Julia one is fairly impressive. But the community is working on it (see the fortran-lang website or the Fortran discourse if you don’t trust me). The bad rep of Fortran is unjustified, particularly for teaching purposes. Many of its detractors have hardly been exposed to anything else than FORTRAN 77. And it’s true that, by current standards, most of FORTRAN 77 codes are terrible sphagetti codes making extensive use of implicit typing and incomprehensible goto statements. Even I, as a Fortran programmer, acknowledge it. But that’s no longer what Fortran is since the 1990’s, and certainly not today!"
  },
  {
    "objectID": "posts/blog-title/first_step_with_elan.html",
    "href": "posts/blog-title/first_step_with_elan.html",
    "title": "First steps with Elan, an educational programming language",
    "section": "",
    "text": "A few days after discussing the merits of Fortran for teaching numerical linear algebra, I got curious as to what is the state of educational programming languages.\nfunction solve(r as Array2D&lt;of Float&gt;, b as Array&lt;of Float&gt;) returns Array&lt;of Float&gt;\n  let n be r.columns()\n  variable x set to new Array&lt;of Float&gt;(n, 0.0)\n  for i from n - 1 to 0 step -1\n    set x to x.withPut(i, b[i])\n    for j from n - 1 to i + 1 step -1\n      set x to x.withPut(i, x[i] - r[j, i]*x[j])\n    end for\n    set x to x.withPut(i, x[i]/r[i, i])\n  end for\n  return x\nend function\n\nmain\n  let n be 3\n  variable r set to new Array2D&lt;of Float&gt;(n, n, 0.0)\n  variable b set to new Array&lt;of Float&gt;(n, 0.0)\n  # Enter the matrix R.\n  call r.put(0, 0, 1.0)\n  call r.put(1, 0, -2.0)\n  call r.put(2, 0, 1.0)\n  call r.put(1, 1, 1.0)\n  call r.put(2, 1, 6.0)\n  call r.put(2, 2, 1.0)\n  print r\n  # Enter the right-hand side vector b.\n  call b.put(0, 4.0)\n  call b.put(1, -1.0)\n  call b.put(2, 2.0)\n  print b\n  # Solve the linear system\n  print solve(r, b)\nend main"
  },
  {
    "objectID": "posts/blog-title/jacobi_experiments.html",
    "href": "posts/blog-title/jacobi_experiments.html",
    "title": "Jacobi method: From a naïve implementation to a modern Fortran multithreaded one",
    "section": "",
    "text": "In a previous post, I used the Jacobi method to illustrate some merits of Fortran over Python for teaching purposes. Since then, I received a handful of messages asking how to write efficient Fortran code. Because of its algorithmic simplicity, the Jacobi method makes for an excellent case study. In this post, we’ll see how to go from a naïve implementation taking a minute to solve a linear system with a quarter million unknowns to a multithreaded version taking less 3 seconds. Bonus point: the code is entirely standard-compliant and you don’t need to know anything about OpenMP or MPI. If you want to see the whole code, check this GitHub repo. But first, what is the Jacobi method?"
  },
  {
    "objectID": "posts/blog-title/jacobi_experiments.html#solving-a-linear-system-with-the-jacobi-method",
    "href": "posts/blog-title/jacobi_experiments.html#solving-a-linear-system-with-the-jacobi-method",
    "title": "Jacobi method: From a naïve implementation to a modern Fortran multithreaded one",
    "section": "Solving a linear system with the Jacobi method?",
    "text": "Solving a linear system with the Jacobi method?\nConsider the system of linear equations\n\\[\n\\mathbf{Ax} = \\mathbf{b},\n\\]\nwhere \\(\\mathbf{A}\\) is an invertible \\(n \\times n\\) matrix. If you ever had a course on numerical linear algebra, you have seen various algorithms to solve this problem. These are divided in two categories: direct solvers targetting small- to medium-sized dense matrices, and iterative solvers for large sparse matrices.\nAmong the zoo of iterative methods, the Jacobi method is probably the first one you’ve encountered. There are two reasons for that:\n\nIt is easy to implement, no matter the programming language.\nIts theoretical analysis is rather simple, even for undergrad students.\n\nIt does come with its limitations though: it does not work for all possible matrices and the convergence is rather slow (i.e. it requires many iterations). Because of these, the Jacobi method is not a viable alternative compared to the (preconditioned) conjugate gradient or multigrid methods and is thus hardly used in production codes. It is however, in my opinion, a fantastic learning example. So, how does it work?\n\nA brief overview\nThe Jacobi method relies on the additive decomposition:\n\\[\\mathbf{A} = \\mathbf{D} + \\mathbf{R},\\]\nwhere \\(\\mathbf{D}\\) is the diagonal component of \\(\\mathbf{A}\\), and \\(\\mathbf{R}\\) consists of the off-diagonal terms. Plugging this decomposition into our system leads to\n\\[\n\\mathbf{Dx} + \\mathbf{Rx} = \\mathbf{b}.\n\\]\nStarting from an initial guess \\(\\mathbf{x}_0\\), the core idea of the Jacobi method is to treat the diagonal contributions implicitly and the off-diagonal ones explicitly, analoguous to a time-integration scheme. This leads to the following iterative scheme\n\\[\n\\mathbf{x}_{t+1} = \\mathbf{D}^{-1} \\left( \\mathbf{b} - \\mathbf{Rx}_t \\right),\n\\]\nwhere subscript \\(t\\) denotes the \\(t\\)-th iteration of the method. What we claim then is that, under suitable assumptions on \\(\\mathbf{A}\\), the iterate \\(\\mathbf{x}_t\\) converges to the actual solution of the system as \\(t \\to \\infty\\). So when does it converge? And if so, how fast does it converge?1\n\n\nFair enough, but does it actually converge?\nThe questions of whether or not an iterative method converges and, if so, how fast does it converge are obviously critical to assess its competitiveness. To answer to both of these questions, let us rewrite the Jacobi iteration as\n\\[\n\\begin{aligned}\n\\mathbf{x}_{t+1}    &   = \\mathbf{x}_t - \\mathbf{D}^{-1} \\left( \\mathbf{b} - \\mathbf{Ax}_t \\right) \\\\\n                    &   = \\left( \\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A} \\right) \\mathbf{x}_t - \\mathbf{D}^{-1} \\mathbf{b}.\n\\end{aligned}\n\\]\nTo derive this expression, simply add and subtract \\(\\mathbf{Dx}_t\\) inside the parenthesized term in the right-hand side and group terms together. Now, let \\(\\mathbf{x}_{\\star}\\) be the true solution of the system, i.e. \\(\\mathbf{x}_{\\star} = \\mathbf{A}^{-1} \\mathbf{b}\\), and \\(\\mathbf{e}_t = \\mathbf{x}_t - \\mathbf{x}_{\\star}\\) be the error at iteration \\(t\\). Using simple algebraic manipulations, the dynamics of the error vector are governed by\n\\[\n\\mathbf{e}_{t+1} = \\left(\\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\right) \\mathbf{e}_t.\n\\]\nObviously, the Jacobi method converges to the correct solution provided \\(\\displaystyle \\lim_{t \\to \\infty} \\| \\mathbf{e}_t \\| = 0\\) where \\(\\| \\cdot \\|\\) is a suitable vector norm. The question of its convergence thus reduces to: under what condition on \\(\\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}\\) does the norm of the error vector goes to zero?\n\nTheorem n°1 – The Jacobi iterative method\n\\[\\mathbf{x}_{t+1} = \\left( \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\right) \\mathbf{x}_t - \\mathbf{D}^{-1}\\mathbf{b}\\]\nconverges for any initial vector \\(\\mathbf{x}_0\\) provided \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| &lt; 1\\) where \\(\\| \\cdot \\|\\) is a matrix norm induced by the corresponding vector norm.\n\n\nSketch of the proof – Let \\(\\| \\cdot \\|\\) be a matrix norm consistent with a vector norm. Then\n\\[\\| \\mathbf{e}_{t+1} \\| = \\| \\left( \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\right) \\mathbf{e}_t \\| \\leq \\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| \\cdot \\| \\mathbf{e}_t \\|.\\]\nA simple inductive argument shows that (for \\(t\\) large enough)\n\\[\\| \\mathbf{e}_t \\| \\leq \\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\|^t \\cdot \\| \\mathbf{e}_0 \\|.\\]\nHence, \\(\\| \\mathbf{e}_t \\|\\) converges to zero as \\(t \\to \\infty\\) for all \\(\\mathbf{e}_0\\) provided that \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| &lt; 1\\).\n\nAlright, we now know the Jacobi method converges provided \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| &lt; 1\\). But what are the necessary and/or sufficient conditions on \\(\\mathbf{A}\\) for \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\|\\) to be less than unity?\n\nTheorem n°2 – Let \\(\\mathbf{A}\\) and \\(2\\mathbf{D} - \\mathbf{A}\\) be symmetric positive definite matrices. Then, the Jacobi iteration converges.\n\nThe proof is divided in two parts.\n\nProof (part 1) – Let \\(\\mathbf{A}\\) be symmetric positive definite and \\(\\mu\\) be an eigenvalue of \\(\\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}\\) with eigenvector \\(\\mathbf{v}\\). Then \\[ \\left( \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\right) \\mathbf{v} = \\mu \\mathbf{v}.\\] Mutliplying from the left by \\(\\mathbf{D}^{-1}\\) leads to \\[ \\left( \\mathbf{D} - \\mathbf{A} \\right) \\mathbf{v} = \\mu \\mathbf{Dv}.\\] Then \\[ \\mathbf{v}^T \\left( \\mathbf{D} - \\mathbf{A} \\right) \\mathbf{v} = \\mu \\mathbf{v}^T \\mathbf{Dv}.\\] Re-arranging terms yields \\[ \\left(1 - \\mu \\right) \\mathbf{v}^T \\mathbf{Dv} = \\mathbf{v}^T \\mathbf{Av}.\\] \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) being symmetric positive definite, we have \\[ \\mathbf{v}^T \\mathbf{Av} &gt; 0 \\quad \\text{and} \\quad \\mathbf{v}^T \\mathbf{Dv} &gt; 0.\\] It implies \\(\\left( 1 - \\mu \\right) &gt; 0\\) and thus \\(\\mu &lt; 1\\). Hence, all the eigenvalues \\(\\mu\\) of \\(\\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}\\) are less than unity.\n\nWhile we arrived at the conclusion that \\(\\mu &lt; 1\\), nothing so far implies \\(-1 &lt; \\mu\\) and thus \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| &lt; 1\\). This is where the condition on \\(2\\mathbf{D} - \\mathbf{A}\\) comes into play.\n\nProof (part 2) – Let \\(2\\mathbf{D} - \\mathbf{A}\\) be symmetric positive definite. Then \\[\\mathbf{v}^T \\left( 2\\mathbf{D} - \\mathbf{A} \\right) \\mathbf{v} &gt; 0\\] and thus \\[\\mathbf{v}^T \\left( \\mathbf{D} - \\mathbf{A} \\right) \\mathbf{v} &gt; - \\mathbf{v}^T \\mathbf{Dv}.\\] From part 1, we know that \\(\\mathbf{v}^T \\left( \\mathbf{D} - \\mathbf{A} \\right) \\mathbf{v} = \\mu \\mathbf{v}^T \\mathbf{Dv}\\). Hence \\[\\mu \\mathbf{v}^T \\mathbf{Dv} &gt; - \\mathbf{v}^T\\mathbf{Dv}\\] implying \\(-1 &lt; \\mu\\). Combined with part 1, we thus have \\(-1 &lt; \\mu &lt; 1\\), i.e. the eigenvalues of \\(\\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A}\\) are inside the unit circle (and real) and the Jacobi iteration converges.\n\nNote that the condition “\\(\\mathbf{A}\\) and \\(2\\mathbf{D} - \\mathbf{A}\\) being symmetric positive definite” is sufficient although not necessary to guarantee the convergence of the Jacobi method. Another classical sufficient but not necessary condition is that \\(\\mathbf{A}\\) is strictly row diagonally dominant. Again, it is relatively easy to prove but since I’m the teacher here, I’ll end this theoretical analysis with the nefarious: This is left as an exercise for the reader.\n\n\nAlright, it converges. But how fast?\nWe’ve actually already partially answered this question. From the sketch of the proof for Theorem n°1, we have\n\\[\n\\| \\mathbf{e}_t \\| \\leq \\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\|^t \\cdot \\| \\mathbf{e}_0 \\|.\n\\]\nObviously, the smaller \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\|\\), the faster the convergence. And we know that \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\| &lt; 1\\) is related to the eigenvalues of the iteration matrix being inside the unit circle. So how do the eigenvalues influence the convergence rate of the method?\nA useful quantity to estimate the convergence rate of the method is the spectral radius of the iteration matrix \\(\\mathbf{M} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}\\). It is defined as\n\\[\n\\rho(\\mathbf{M}) = \\max \\left\\{ \\vert \\mu_1 \\vert, \\cdots, \\vert \\mu_n \\vert \\right\\},\n\\]\nwhere \\(\\mu_i\\) are the eigenvalues. Moreover, \\(\\rho(\\mathbf{M}) \\leq \\| \\mathbf{M} \\|\\) for every natural matrix norms. From our previous discussion, we know that \\(\\rho(\\mathbf{M}) &lt; 1\\) since the Jacobi method converges. Eventhough the spectral radius is only a lower bound for \\(\\| \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A} \\|\\), we’ll assume for the sake of simplicity that it is pretty tight. Hence, we roughly have\n\\[\n\\| \\mathbf{e}_t \\| \\leq \\rho(\\mathbf{M})^t \\cdot \\| \\mathbf{e}_0 \\|.\n\\]\nWe could make this statement more formal but it wouldn’t change the intuition: the smaller the spectral radius, the larger the asymptotic convergence rate. Unfortunately, there is not much else to say without knowing exactly the matrix \\(\\mathbf{A}\\) so let’s turn to the actual problem of interest of this post."
  },
  {
    "objectID": "posts/blog-title/jacobi_experiments.html#the-poissons-equation-on-the-unit-square",
    "href": "posts/blog-title/jacobi_experiments.html#the-poissons-equation-on-the-unit-square",
    "title": "Jacobi method: From a naïve implementation to a modern Fortran multithreaded one",
    "section": "The Poisson’s equation on the unit square",
    "text": "The Poisson’s equation on the unit square\nThe Poisson’s equation is an elliptic partial differential equation (PDE) appearing in numerous fields of physics. Let’s parse what this means:\n\nPDE – The solution of the equation depends on more than one variable, where the variables are typically the different spatial dimensions.\nElliptic – The solution exhibits a certain notion of smoothness, whatever that means mathematically. Typically, it implies that the solution will not exhibit any discontinuities or very steep fronts in contrast to what you may see for hyperbolic equations (think shock waves for instance).\n\nMathematically, it reads\n\\[\n\\nabla^2 u = -f,\n\\]\nalong with appropriate boundary conditions. In the rest of this post, we’ll consider one of its simplest variations. The domain \\(\\Omega\\) is the unit square, i.e. \\(\\Omega = \\left[0, 1 \\right]^2\\) and we will consider only homogenous Dirichlet boundary conditions, i.e. \\(u = 0\\) on the boundaries of the square. Our problem thus is\n\\[\n\\left\\{\n\\begin{aligned}\n    \\dfrac{\\partial^2 u}{\\partial x^2} + \\dfrac{\\partial^2 u}{\\partial y^2} = -f \\quad & \\text{for } (x, y) \\in \\Omega \\\\\n    u(x, y) = 0 \\quad & \\text{for } (x, y) \\in \\partial \\Omega.\n\\end{aligned}\n\\right.\n\\]\nNote that the problem is sufficiently simple that you can express its analytical solution using Fourier series. But we are computational scientists, so we’ll solve the problem numerically.\n\nDiscretizing the problem\nThere are many different ways to discretize a partial differential equation. Finite differences, finite volumes, finite elements, spectral elements, spectral methods, pseudo-spectral methods, etc. There are no silver bullets though. Each has its pros and cons. At the end of the day, the discretization method used is often a matter of personal preferences. To keep things simple, we will consider the standard second-order accurate finite-difference scheme. We will also consider a uniform grid spacing in each direction so that our differential operators can be approximated as\n\\[\n\\dfrac{\\partial^2 u}{\\partial x^2} \\simeq \\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\\Delta x^2}\n\\quad \\text{and} \\quad\n\\dfrac{\\partial^2 u}{\\partial y^2} \\simeq \\dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\\Delta y^2}\n\\]\nwhere \\(\\Delta x\\) and \\(\\Delta y\\) are the grid sizes in each direction, and \\(u_{i, j}\\) is the value of our unknown function evaluated at the grid point \\((x_i, y_j) = (i \\Delta x, j \\Delta y)\\). For the sake of simplicity, we’ll assume furthermore that \\(\\Delta x = \\Delta y\\). Our discretized partial differential equation for points inside of the domain then reads\n\\[\n\\dfrac{1}{\\Delta x^2} \\left( u_{i+1, j} + u_{i-1, j} + u_{i, j+1} + u_{i, j-1} - 4 u_{i, j} \\right) = -f_{i, j}.\n\\]\nIt may not seem like a linear system, but trust me, it is. The field \\(u(x, y)\\) is represented as a two-dimensional array (and I mean array, not matrix) for the sake of simplicity. But you can always represent it as a vector by simply stacking the columns of the array on top of one another, and likewise for the right-hand side forcing \\(f(x, y)\\).\nSo, where is the matrix then? Consider a single column of the array \\(u\\), that is we fix \\(x\\) and only consider different \\(y\\)-values. The second-order derivative in the \\(y\\)-direction can be represented as an \\((n_y-2) \\times (n_y-2)\\) matrix given by\n\\[\n\\mathbf{D}_y\n=\n\\dfrac{1}{\\Delta y^2}\n\\begin{bmatrix}\n    -2  & 1 \\\\\n    1   & -2 & 1 \\\\\n        & 1 & -2 & 1 \\\\\n        &  & \\ddots & \\ddots & \\ddots \\\\\n        & &   &   1   &   -2  &   1   \\\\\n        & &   &       &   1   & -2\n\\end{bmatrix}\n\\]\nwhere we excluded the points on the upper and lower boundaries as these are equal to zero owing to our choice of boundary conditions. Likewise, considering a single row of \\(u\\) (i.e. fixing \\(y\\) and considering different \\(x\\)-values), the second-order derivative in the horizontal direction can be represented as an \\((n_x - 2) \\times (n_x -2)\\) matrix \\(\\mathbf{D}_x\\) with the same tridiagonal structure as \\(\\mathbf{D}_y\\). Our problem can then be represented in a standard linear system form as\n\\[\n\\left( \\mathbf{I}_{n_y} \\otimes \\mathbf{D}_x + \\mathbf{D}_y \\otimes \\mathbf{I}_{n_x} \\right) \\mathrm{vec}(u) = -\\mathrm{vec}(f),\n\\]\nwhere \\(\\otimes\\) is the Kronecker product. If we were to explicitly construct it, this matrix would have \\(n_x \\times n_y\\) rows and likewise for the number of columns. For a discretization employing 512 points in each direction, that would be 260 100 columns and rows. Pretty big then, and completely intractable for standard direct linear solvers!\n\n\nThe Jacobi method for the 2D Poisson equation\nTime to write the Jacobi update rule for our particular problem. Recall that our problem reads\n\\[\n\\dfrac{1}{\\Delta x^2} \\left( u_{i+1, j} + u_{i-1, j} + u_{i, j+1} + u_{i, j-1} - 4 u_{i, j} \\right) = -f_{i, j}.\n\\]\nOn the left-hand side, the \\(u_{ij}\\) term corresponds to the diagonal component while all the others are the off-diagonal ones. Following what we have written for the Jacobi method in matrix form, specializing for this equation leads to the following update rule\n\\[\nu_{i, j}^{(t+1)} = \\dfrac{1}{4} \\left( \\Delta x^2 \\cdot f_{i, j} - u_{i+1, j}^{(t)} - u_{i-1, j}^{(t)} - u_{i, j+1}^{(t)} - u_{i, j-1}^{(t)} \\right)\n\\]\nwhere the superscript \\(\\cdot ^{(t)}\\) denotes the iteration number. This will be fairly simple to implement. Create two arrays, one to store the solution at iteration \\(t\\) and the other one at iteration \\(t+1\\). Loop over the indices and update the \\((i, j)\\)-th entries of the second table with the appropriate combination of values from the first one. Note that it is important to keep these two tables. If you were to have only one table and directly update its \\((i, j)\\) entry, you would end-up with a different method: Gauss-Seidel. More on that in a later post (maybe).\nConvergence properties – The second-order accurate central finite-difference approximation \\(\\mathbf{L} = \\mathbf{I}_{n_y} \\otimes \\mathbf{D}_x + \\mathbf{D}_y \\otimes \\mathbf{I}_{n_x}\\) of the Laplace operator \\(\\nabla^2\\) is a symmetric negative definite matrix. Hence, \\(-\\mathbf{L}\\) is symmetric positive definite (and this is the reason for why there is a minus sign on the right-hand side of our problem if you wondered). It is easy to show moreover that \\(-\\mathbf{L}\\) satisfies the assumptions for Theorem n°2 to hold. Hence, after a sufficiently large number of iterations, the Jacobi method will converge to the actual solution of our linear system. But again, how fast?\nAs before, we can get some intuition by looking at the spectral radius of this matrix. I won’t go through the calculations (and I’ll assume \\(n_x = n_y\\)), but we basically have\n\\[\n\\rho(\\mathbf{I} + \\mathbf{D}^{-1} \\mathbf{L}) \\simeq 1 - \\dfrac{\\pi^2}{2 n_x^2}.\n\\]\nUsing an increasing number of grid points to discretize our domain (that is considering a finer and finer mesh), the spectral radius of the iteration matrix gets closer and closer to unity. As a consequence, the Jacobi method requires more and more iterations to compute a reasonnably accurate solution. This poor scaling property is one of the reasons why it ain’t actually used nowadays in high-performance computing solvers. But this does not concern us here."
  },
  {
    "objectID": "posts/blog-title/jacobi_experiments.html#let-fortran-shine",
    "href": "posts/blog-title/jacobi_experiments.html#let-fortran-shine",
    "title": "Jacobi method: From a naïve implementation to a modern Fortran multithreaded one",
    "section": "Let Fortran shine!",
    "text": "Let Fortran shine!\nAlright! It’s time for what you all expected: the Fortran implementation. We’ll start with a simple translation to Fortran of the pseudo-code. This implementation will be our baseline. We will then incrementally improve it by using various tips and tricks with a particular constraint: use only standard-compliant Fortran code. I’ll try to explain the rationale behind every decision I make along the way. By the end of our journey, we’ll have a standard-compliant implementation which can naturally leverage multithreaded computations without having have to write a single openMP pragma. Performance-wise, we’ll end up with a 20x to 30x speed-up compared to our baseline implementation without every leaving the realm of Fortran. Too good to be true? Bare with me then!\n\nBaseline implementation\nLet us start with an almost verbatim translation of the pseudo-code to Fortran. In the rest, we will use double precision arithmetic. The kind parameter will be defined as\ninteger, parameter :: dp = selected_real_kind(15, 307)\nThis is often considered to be a good practice in Fortran and guarantees a certain portability of the code across different compilers and platforms. Let us now turn our attention to the Jacobi kernel. Our textbook implementation is shown below.\npure subroutine textbook_kernel(nx, ny, u, v, b, dx)\n    implicit none\n    integer, intent(in) :: nx, ny\n    real(dp), intent(out) :: u(nx, ny)\n    real(dp), intent(in) :: v(nx, ny), b(nx, ny), dx\n    integer :: i, j\n    do j = 2, ny-1\n        do i = 2, nx-1\n            u(i, j) = 0.25_dp*(b(i, j)*dx**2 - v(i+1, j) - v(i-1, j) &\n                                             - v(i, j+1) - v(i, j-1))\n        enddo\n    enddo\nend subroutine\nThe pure keyword is here to tell the compiler that we guarantee this subroutine has no unintended side-effect. It is not technically mandatory, but it is also part of the good practices in Fortran. Hopefully, if we do things right, this kernel should be where we spend most of the computational time. To the actual solver now.\nfunction textbook_solver(b, tol, maxiter) result(u)\n    implicit none\n    real(dp), intent(in) :: b(:, :), tol\n    integer, intent(in) :: maxiter\n    real(dp), allocatable :: u(:, :)\n    ! Internal variables\n    integer :: nx, ny, i, j, iteration\n    real(dp), allocatable :: v(:, :)\n    real(dp) :: dx, l2_norm\n    ! Initialize variables\n    nx = size(b, 1); ny = size(b, 2); dx = 1.0_dp/(nx - 1)\n    if (nx /= ny) then\n        error stop \"Number of points in each direction need to be equal.\"\n    endif\n    allocate (u(nx, ny), source=0.0_dp)\n    allocate (v(nx, ny), source=0.0_dp)\n    l2_norm = 1.0_dp\n    iteration = 0\n    ! Begining of the Jacobi iterative method.\n    do while ((iteration &lt; maxiter) .and. (l2_norm &gt; tol))\n        ! Jacobi iteration.\n        call textbook_kernel(nx, ny, v, u, b, dx)\n        ! Compute error norm.\n        l2_norm = norm2(u - v)\n        ! Update variable.\n        u = v\n        ! Update iteration counter.\n        iteration = iteration + 1\n    end do\n    print *, \"Textbook solver :\"\n    print *, \"    - Number of iterations :\", iteration\n    print *, \"    - l2-norm of the error :\", l2_norm\nend function\nEven if you ain’t familiar with Fortran, the code should be quite readable. After having declared and initialized all of the required variables, the Jacobi method starts from Line 20 and proceeds in 3 steps:\n\nPerform the Jacobi update by calling our textbook kernel.\nCompute the 2-norm of the correction.\nUpdate the current solution with its latest estimate.\n\nThis loop keeps on going until the 2-norm of the correction is small enough to claim convergence. In all of our experiments, the tolerance is set to \\(10^{-8}\\).\nPerformances – We will use 512 points in each direction with a uniform grid spacing and assume the initial guess to be the zero solution for all of our experiments. We thus have slightly more than a quarter million of unknowns, a reasonnably large linear system. The code is compiled using gfortran 15.1 and the following options: -O3 -march=native -mtune=native. The table below summarizes some of the key computational metrics.\n\n\n\n\n\n\n\n\n\nSolver\n# of iterations\nTime to solution\nSpeed-up w.r.t. baseline\n\n\n\n\nTextbook\n128 395\n58 s\n1\n\n\n\nSolving a linear system with a quarter million of unknowns in under one minute is quite impressive when you think about it. It is clearly orders of magnitude faster than if you were to do it by hand (and far less error-prone)! But is this the best we can do? You might be inclined to say yes. After all, our implementation is an almost verbatim translation of the pseudo-code and maths don’t lie. But that ain’t completely true though… When it comes to scientific computing, there are many streetfighting skills you can pick along the way to massively improve the computational performances of a given algorithm. So let’s start optimizing!\n\n\nYou shall not copy!\nOur Jacobi kernel is so simple that there ain’t much room for improvement so let’s look at the solver itself starting with line 26\n    u = v\nIt is the update of our current estimate of the solution with the one we’ve just computed. It essentially is a copy operation. Given how simple our Jacobi kernel is, it acutally takes almost as long as computing a Jacobi update. So let’s get rid of it by simply performing an additional call to the Jacobi kernel with role of u and v being flipped.\nfunction nocopy_solver(b, tol, maxiter) result(u)\n    implicit none\n    real(dp), intent(in) :: b(:, :), tol\n    integer, intent(in) :: maxiter\n    real(dp), allocatable :: u(:, :)\n    ! Internal variables\n    integer :: nx, ny, i, j, iteration\n    real(dp), allocatable :: v(:, :)\n    real(dp) :: dx, l2_norm\n    ! Initialize variables\n    nx = size(b, 1); ny = size(b, 2); dx = 1.0_dp/(nx - 1)\n    if (nx /= ny) then\n        error stop \"Number of points in each direction need to be equal.\"\n    endif\n    allocate (u(nx, ny), source=0.0_dp)\n    allocate (v(nx, ny), source=0.0_dp)\n    l2_norm = 1.0_dp\n    iteration = 0\n    ! Begining of the Jacobi iterative method.\n    do while ((iteration &lt; maxiter) .and. (l2_norm &gt; tol))\n        ! Jacobi iteration (no copy).\n        call textbook_kernel(nx, ny, v, u, b, dx)\n        call textbook_kernel(nx, ny, u, v, b, dx)\n        ! Compute error norm.\n        l2_norm = norm2(u - v)\n        ! Update variable.\n        u = v\n        ! Update iteration counter.\n        iteration = iteration + 2\n    end do\n    print *, \"No-copy solver  :\"\n    print *, \"    - Number of iterations :\", iteration\n    print *, \"    - l2-norm of the error :\", l2_norm\nend function\nPerformances – Code-wise, very little has changed compared to our baseline implementation. The nocopy_solver slightlty departs from the pseudo-code but is still as readable. Peformance-wise, it is a different story.\n\n\n\n\n\n\n\n\n\nSolver\n# of iterations\nTime to solution\nSpeed-up w.r.t. baseline\n\n\n\n\nTextbook\n128 395\n58 s\n1\n\n\nNo-copy\n128 396\n30 s\n1.9\n\n\n\nThis one-line change makes our solver compute the solution twice as fast! But don’t get too excited, it is somewhat expected if you think of it. A copy is roughly as expensive as computing a Jacobi update itself. As a consequence, in the time frame it took our baseline implementation to peform a Jacobi update followed by a copy, the nocopy_solver performed no copy (hence the name) but two updates. And boom, twice as fast. This is the first but probably most important take-away message:\n\nAvoir copies like the plague and re-use intermediate results as much as possible.\n\nThis is not specific to Fortran and is true for pretty much any programming language you use.\n\n\nFurther optimizations\nWhile the no-copy trick is fairly general, let us turn now to somewhat Jacobi-specific optimization tricks starting with the Jacobi kernel itself. Let \\(v\\) be the current approximate solution and \\(u\\) the new one being computed. Recall that the update rule is as follows\n\\[\n    u_{i, j} = \\dfrac{1}{4} \\left( b_{i, j} \\cdot \\Delta x^2 - (v_{i+1, j} + v_{i-1, j} + v_{i, j+1} + v_{i, j-1}) \\right)\n\\]\nfor all \\(i\\) and \\(j\\) corresponding to points in the interior of the computational domain. One crucial observation is that there are no dependencies between the entries of \\(u\\): they can be update in any abitrary order, not necessarily the lexicographic one. In particular, we could let the compiler decide on its own what is the most efficient way to do this update based on its internal mechanics. The 2008 standard introduced a particular construct conveying precisely this: the do concurrent. Below is the Jacobi kernel rewritten using this construct.\npure subroutine doconcurrent_kernel(nx, ny, u, v, b, dx)\n    implicit none(external)\n    integer(ilp), intent(in) :: nx, ny\n    real(dp), intent(out) :: u(nx, ny)\n    real(dp), intent(in) :: v(nx, ny), b(nx, ny), dx\n    integer :: i, j\n    do concurrent(i=2:nx - 1, j=2:ny - 1)\n        ! Jacobi update.\n        u(i, j) = 0.25_dp*(b(i, j)*dx**2 - v(i + 1, j) - v(i - 1, j) &\n                                         - v(i, j + 1) - v(i, j - 1))\n    end do\nend subroutine doconcurrent_kernel\nFor now, it does not actually improve the computational performances of our kernel. For serial computations, it mostly is a syntactic sugar letting someone reading the code know that this loop could technically be computed in parallel with no problem. It might help the compiler optimize a bit, but the kernel being so simple I haven’t seen much changes. It’ll be different though once we go to multithreaded computations but that’s a story for slightly later.\nThe main source of computational improvement is located on line 25:\n    l2_norm = norm2(u-v)\nThere is nothing particularly wrong with this line. In practice however, the Jacobi method is quite slow to converge and computing the residual norm at every iteration incurs extra computational costs which are unecessary. We would be much better off by checking the residual only once in a while. We could replace it with\n    if (mod(iteration, 1000)) l2_norm = norm2(u - v)\nor using the newest do concurrent construct\nif (mod(iteration, 1000)) then\n    l2_norm = 0.0_dp\n    do concurrent(i=2:nx-1, j=2:ny-1) reduce(+:l2_norm)\n        l2_norm = l2_norm + (u(i, j) - v(i, j))**2\n    enddo\n    l2_norm = sqrt(l2_norm)\nendif\nEither way is fine, norm2 is an intrinsic Fortran function and its implementation has already been optimized by the compiler vendors. Checking the residual norm every 1000 iterations is arbitrary. It has been chosen out of simplicity considering that the method takes 128 000 iterations to converge on our particular problem. In practice, you might actually pass this as an extra argument to the solver to let the user decide. Here is the updated solver.\nfunction doconcurrent_solver(b, tol, maxiter) result(u)\n    implicit none(external)\n    real(dp), intent(in) :: b(:, :), tol\n    integer, intent(in) :: maxiter\n    real(dp), allocatable :: u(:, :)\n    ! Internal variables.\n    integer :: nx, ny, i, j, iteration\n    real(dp), allocatable :: v(:, :)\n    real(dp) :: dx, l2_norm\n    ! Initialize variables\n    nx = size(b, 1); ny = size(b, 2); dx = 1.0_dp/(nx - 1)\n    if (nx /= ny) then\n        error stop \"Number of points in each direction need to be equal.\"\n    endif\n    allocate (u(nx, ny), source=0.0_dp)\n    allocate (v(nx, ny), source=0.0_dp)\n    l2_norm = 1.0_dp\n    iteration = 0\n    do while ((iteration &lt; maxiter) .and. (l2_norm &gt; tol))\n        ! Jacobi kernel (no-copy).\n        call doconcurrent_kernel(nx, ny, v, u, b, dx)\n        call doconcurrent_kernel(nx, ny, u, v, b, dx)\n        ! Compute error norm.\n        if (mod(iteration, 1000) == 0) l2_norm = error_norm(u, v)\n        ! Update iteration counter.\n        iteration = iteration + 2\n    end do\n    l2_norm = error_norm(u, v) ! Sanity check\n    print *, \"Do-concurrent solver :\"\n    print *, \"    - Number of iterations :\", iteration\n    print *, \"    - l2-norm of the error :\", l2_norm\nend function\n\npure real(dp) function error_norm(u, v) result(l2_norm)\n    implicit none\n    real(dp), intent(in) :: u(:, :), v(:, :)\n    integer :: i, j\n    l2_norm = 0.0_dp\n    do concurrent(i=2:size(u, 1)-1, j=2:size(u, 2)-1) reduce(+:l2_norm)\n        l2_norm = l2_norm + (u(i, j) - v(i, j))**2\n    end do\n    l2_norm = sqrt(l2_norm)\nend function\nPerformances – Again, the new solver is just as readable as the previous ones. No big changes here, but look at the performances below!\n\n\n\n\n\n\n\n\n\nSolver\n# of iterations\nTime to solution\nSpeed-up w.r.t. baseline\n\n\n\n\nTextbook\n128 395\n58 s\n1\n\n\nNo-copy\n128 396\n30 s\n1.9\n\n\ndo concurrent\n129 002\n16 s\n3.6\n\n\n\nThe new solver is 3 to 4 times faster than our baseline! This is quite remarkable given that we changed only a couple of lines compared to the original textbook implementation. Things are not always so clear cut for more complex algorithms, but still. And here is our second take-way:\n\nCompute just what you need, not more.\n\nHere, computing the residual norm for each iteration was clearly a non-negligible and non-necessary bottleneck. At this point, there is no more low-hanging fruit for optimization. You might think this is it. A 3.6x speed-up is good enough and you may call it a day. But you know what? There’s more. We can reach a 20x to 30x speed-up without changing anything else to the code!\n\n\nMultithreaded performances\nComputers these days tend to have built-in parallel computing capabilities. Yet, we haven’t leverage these so far. Let’s change that. I will not get into a discussion about openMP vs MPI or GPU offloading. I will keep things very practical instead. Remember when I said we can perform the Jacobi update in any \\(i, j\\) order we want? The Jacobi update rule is embarassingly parallel. That is precisely what the do concurrent construct is conveying. And compilers can leverage this for increased computational performances. For our solver, it is a simple as changing the gfortran compilation options from\n-O3 -mtune=native -march=native\nto\n-O3 -mtune=native -march=native -ftree-parallelize-loop=n\nwhere n is the number of processes/threads to be used. And that’s it. Litterally. And look at these performances!\n\n\n\n\n\n\n\n\nNumber of threads\nTime to solution\nSpeed-up w.r.t. baseline\n\n\n\n\n1\n16 s\n3.6\n\n\n2\n8.6 s\n6.7\n\n\n4\n4.1 s\n14.1\n\n\n8\n2.1 s\n27.6\n\n\n\nAs promised, we finish with a linear solver computing the solution of a system with a quarter million of unknowns in less than 3 seconds and not a single openMP pragma or MPI call. The code is the exact same as before. The only thing that changed is the addition of the new compilation option. And that is enough to reach a 27x speed-up compared to our original textbook implementation!2 Pretty good considering we changed only a handful of lines of code and added only one extra compilation option, innit? There would be a lot more to say about the different parallel computing paradigms and the associated neaty greedy details, but this post is already sufficiently long as it is so I’ll stop right there. I’ll leave you with a cautionnary quote by the famous Donald Knuth though\n\nThe real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming."
  },
  {
    "objectID": "posts/blog-title/jacobi_experiments.html#footnotes",
    "href": "posts/blog-title/jacobi_experiments.html#footnotes",
    "title": "Jacobi method: From a naïve implementation to a modern Fortran multithreaded one",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe need to invert the matrix \\(\\mathbf{D}\\) at each iteration. While this operation requires \\(\\mathcal{O}(n^3)\\) flops for a general matrix, \\(\\mathbf{D}\\) here is diagonal. Hence, its inverse is straightforward to compute and only requires \\(n\\) flops. Likewise, the matrix-vector product \\(\\mathbf{Rx}_t\\) requires in general \\(\\mathcal{O}(n^2)\\) flops. In most applications though, the matrix \\(\\mathbf{A}\\) is sparse and so is \\(\\mathbf{R}\\), typically reducing the number of floating points operations down to \\(\\mathcal{O}(n)\\) as well. For a sparse linear system, each iteration of the Jacobi method hence requires \\(\\mathcal{O}(n)\\) flops. The question then is how many iterations does it take to converge?↩︎\nIf you run the code on your own computer, you may get different results as it depends on the number of cores you have, how fast they are, etc. But still, you should get pretty much the same trend.↩︎"
  }
]